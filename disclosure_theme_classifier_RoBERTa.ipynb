{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d30dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use this code to do the following:\n",
    "\n",
    "# 1) Prepare human-coded datset for model building \n",
    "# 2) Train RoBERTa model\n",
    "# 3) Evaluate performance of model\n",
    "# 4) Prepare full dataset for classification\n",
    "# 5) Classify text using trained RoBERTa model\n",
    "\n",
    "### Note that you will need to change your file path names, and change the name of the theme you are working on (i.e. political, humor, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5073fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages ###\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare human-coded datset for model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec42b4c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-48a7745149ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Read the CSV file with specified data types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YOUR FILE PATH HERE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "### Read in the CSV file\n",
    "\n",
    "# Specify data types for specific columns\n",
    "dtype_spec = {\n",
    "    \"id\": str,\n",
    "    \"humorous_attempt\": int,\n",
    "    \"encouraging_others_vaccination\": int,\n",
    "    \"vaccination_experience\": int,\n",
    "    \"side_effects\": int,\n",
    "    \"political\": int,\n",
    "    \"health_benefits\": int,\n",
    "    \"social_benefits\": int,\n",
    "    \"expressing_gratitude_appreciation\": int,\n",
    "    \"expressing_relief\": int,\n",
    "    \"expressing_civic_duty\": int,\n",
    "    \"identifying_vaccine_community\": int,\n",
    "}\n",
    "# Read the CSV file with specified data types\n",
    "df = pd.read_csv(\"YOUR FILE PATH HERE\", dtype=dtype_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset head, columns, or length\n",
    "df.head()\n",
    "#df.columns\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5430287",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to preprocess text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Replace email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Kerning\n",
    "    text = re.sub(r'([A-Z]\\s){2,}[A-Z]', lambda m: m.group().replace(' ', ''), text)\n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "    # Remove #\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    # Replace &amp with \"and\"\n",
    "    text = text.replace(\"&amp\", \"and\")\n",
    "    # Replace date content\n",
    "    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', text)\n",
    "    \n",
    "    # Replace mispelling\n",
    "    replacement_map = {\n",
    "        r'\\bvaccienated\\b': 'vaccinated',\n",
    "    }\n",
    "    for pattern, replacement in replacement_map.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "    }\n",
    "\n",
    "    for contraction, expanded_form in contraction_mapping.items():\n",
    "        text = text.replace(contraction, expanded_form)\n",
    "\n",
    "    # Replace vaccine emoji with \"vaccine\"\n",
    "    text = text.replace(\"\\U0001F48A\", \"vaccine\")  # Replace U+1F48A with \"vaccine\"\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U0001F004\"  # Mahjong Tile Red Dragon\n",
    "                               u\"\\U0001F0CF\"  # Playing Card Black Joker\n",
    "                               u\"\\U0001F170-\\U0001F251\"  # Enclosed Alphanumeric Supplement\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing function to the 'text' column and create a new column 'processed_text'\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bf6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indicate column of interest (the category of which you are going to classify)\n",
    "\n",
    "#Rename the column of interest to \"column_of_interest\"\n",
    "df.rename(columns={'political': 'column_of_interest'}, inplace=True)\n",
    "# Print the modified DataFrame\n",
    "df.head()\n",
    "#Rename the column \"processed_text\" to \"text\"\n",
    "df.rename(columns={'processed_text': 'text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Duplicates\n",
    "#Count duplicates in the 'text' column\n",
    "duplicate_count = df['text'].duplicated().sum()\n",
    "# Print the number of duplicates\n",
    "print(f\"Number of duplicates in 'text' column: {duplicate_count}\")\n",
    "# Print the duplicate instances\n",
    "duplicate_instances = df[df['text'].duplicated(keep=False)]\n",
    "print(\"\\nDuplicate instances:\")\n",
    "print(duplicate_instances)\n",
    "df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfaf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Train RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6647e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build RoBERTa Model\n",
    "\n",
    "# Split the dataframe into training, validation, and test sets (80:10:10 split)###\n",
    "np.random.seed(115)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                                     [int(.8 * len(df)), int(.9 * len(df))])\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "### Define the labels mapping ###\n",
    "labels = {\n",
    "    0: 0,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "### Define a Dataset class to handle the data ###\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['column_of_interest']]\n",
    "        self.texts = [tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) for text in df['text']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.texts[idx]\n",
    "        batch_y = self.labels[idx]\n",
    "\n",
    "        input_ids = batch_texts['input_ids'].squeeze(0)\n",
    "        attention_mask = batch_texts['attention_mask'].squeeze(0)\n",
    "\n",
    "        return input_ids, attention_mask, batch_y\n",
    "\n",
    "### Define the RoBERTa classifier model ###\n",
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f63fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train_dataset = Dataset(train_data)\n",
    "    val_dataset = Dataset(val_data)\n",
    "# Create a DataLoader for the training data\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in tqdm(train_dataloader):\n",
    "            labels = labels.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            batch_loss = criterion(output, labels.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == labels).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_acc_train = total_acc_train / len(train_dataset)\n",
    "        avg_loss_train = total_loss_train / len(train_dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch_num + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {avg_loss_train:.4f}, Train Accuracy: {avg_acc_train:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 35 ### Change Epochs if needed\n",
    "model = RoBERTaClassifier()\n",
    "LR = 1e-5\n",
    "\n",
    "train(model, df_train, df_val, LR, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88830c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save after training\n",
    "torch.save(model.state_dict(), 'roberta_classifier_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64641031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Evaluate performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1585fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test_dataset = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.argmax(dim=1).cpu().numpy()\n",
    "            label_ids = labels.cpu().numpy()\n",
    "            \n",
    "            predictions.extend(logits)\n",
    "            true_labels.extend(label_ids)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test Precision: {precision:.4f}')\n",
    "    print(f'Test Recall: {recall:.4f}')\n",
    "    print(f'Test F1 Score: {f1:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Prepare full dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2366985",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run full dataset (Excluding the training data) through the RoBERTa classifier\n",
    "\n",
    "# Read the CSV file \n",
    "dtype_spec = {\"id\": str}  # Treat the \"id\" column as a string (character) type\n",
    "df2 = pd.read_csv(\"DATA FILE PATH HERE\", dtype=dtype_spec)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete any row that appears in the training set (identified by the id column)\n",
    "# Create a list of ids in df\n",
    "df_ids = df['id'].tolist()\n",
    "# Iterate through new_df and keep only the rows where the \"id\" is not in df_ids\n",
    "filtered_rows = []\n",
    "for index, row in df2.iterrows():\n",
    "    if row['id'] not in df_ids:\n",
    "        filtered_rows.append(row)\n",
    "# Create a new dataframe from the filtered rows\n",
    "new_df = pd.DataFrame(filtered_rows)\n",
    "# Now new_df contains only the rows from new_df that don't appear in df by the \"id\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3df7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing function to the 'text' column and create a new column 'processed_text'\n",
    "new_df['processed_text'] = new_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Classify text using trained RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "\n",
    "loaded_model = RoBERTaClassifier()\n",
    "loaded_model.load_state_dict(torch.load('roberta_classifier_model.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf65bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf122fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify text using the loaded model\n",
    "def classify_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Downloads\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify a batch of texts\n",
    "def classify_batch(batch_texts):\n",
    "    inputs = [tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128) for text in batch_texts]\n",
    "    batch_input_ids = pad_sequence([input_dict['input_ids'].squeeze(0) for input_dict in inputs], batch_first=True)\n",
    "    batch_attention_mask = pad_sequence([input_dict['attention_mask'].squeeze(0) for input_dict in inputs], batch_first=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(batch_input_ids, batch_attention_mask)\n",
    "    batch_predicted_labels = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "    return batch_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing for classification with optimized tokenization\n",
    "def batch_classify_text_optimized(texts, batch_size=16):\n",
    "    num_texts = len(texts)\n",
    "    predicted_labels = []\n",
    "\n",
    "    for i in range(0, num_texts, batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_results = classify_batch(batch_texts)\n",
    "        predicted_labels.extend(batch_results)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing for classification with optimized tokenization and parallel processing\n",
    "def batch_classify_text_parallel_optimized(texts, batch_size=16):\n",
    "    num_texts = len(texts)\n",
    "    predicted_labels = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i in range(0, num_texts, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_results = list(executor.map(classify_batch, [batch_texts]))\n",
    "            predicted_labels.extend(batch_results[0])\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply parallel batch classification with optimized tokenization to the new dataset\n",
    "new_df['predicted_label'] = batch_classify_text_parallel_optimized(new_df['processed_text'], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5953ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output predictions to a CSV file\n",
    "output_filename = \"political_predictions.csv\" ###CHANGE THE NAME HERE\n",
    "new_df.to_csv(output_filename, index=False)\n",
    "print(f\"Predictions saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a77466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CSV file to your local machine\n",
    "files.download(\"political_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af12ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
