{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code used to collect Tweets via the Twitter API\n",
    "\n",
    "import pathlib\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def extract_entities(records):\n",
    "    hashtags = []\n",
    "    urls = []\n",
    "\n",
    "    for tweet in records:\n",
    "        if tweet.entities is None:\n",
    "            tweet.entities = {}\n",
    "        for url_json in tweet.entities.get(\"urls\", []):\n",
    "            url_record = (tweet.id, url_json[\"expanded_url\"])\n",
    "            urls.append(url_record)\n",
    "\n",
    "        for hashtag_json in tweet.entities.get(\"hashtags\", []):\n",
    "            hashtag_record = (tweet.id, hashtag_json[\"tag\"])\n",
    "            hashtags.append(hashtag_record)\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(hashtags, columns=[\"tweet_id\", \"hashtag\"]),\n",
    "        pd.DataFrame(urls, columns=[\"tweet_id\", \"hashtag\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_referenced(records):\n",
    "    referenced = []\n",
    "    for tweet in records:\n",
    "        for reference in tweet.data.get(\"referenced_tweets\", []):\n",
    "            ref_record = (tweet.id, reference[\"type\"], reference[\"id\"])\n",
    "            referenced.append(ref_record)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        referenced, columns=[\"tweet_id\", \"reference_type\", \"reference_id\"]\n",
    "    )\n",
    "\n",
    "\n",
    "BEARER_TOKEN = \"YOUR BEARER TOKEN HERE\"\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Arguments for the request\n",
    "# Details for building query parameters can be found here:  https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "# Change query parameters for each API pull\n",
    "query= \"\"\"(#vaccinated) -flu -flushot -fluvaccine -hpv -influenza -smallpox -is:retweet -is:quote -is:nullcast lang:en\"\"\"\n",
    "\n",
    "# YYYY-MM-DDTHH:mm:ssZ\n",
    "start_time = \"2021-12-14T00:00:00Z\"\n",
    "end_time = \"2022-01-01T00:00:00Z\"\n",
    "\n",
    "# https://developer.twitter.com/en/docs/twitter-api/expansions\n",
    "expansions = [\"author_id\", \"geo.place_id\", \"attachments.media_keys\"]\n",
    "\n",
    "# https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model\n",
    "place_fields = [\"country\", \"country_code\"]\n",
    "media_fields = [\"media_key\", \"type\", \"preview_image_url\", \"public_metrics\"]\n",
    "user_fields = [\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"username\",\n",
    "    \"created_at\",\n",
    "    \"description\",\n",
    "    \"profile_image_url\",\n",
    "    \"public_metrics\",\n",
    "    \"url\",\n",
    "    \"verified\",\n",
    "]\n",
    "tweet_fields = [\n",
    "    \"attachments\",\n",
    "    \"author_id\",\n",
    "    \"conversation_id\",\n",
    "    \"created_at\",\n",
    "    \"entities\",\n",
    "    \"geo\",\n",
    "    \"id\",\n",
    "    \"in_reply_to_user_id\",\n",
    "    \"lang\",\n",
    "    \"public_metrics\",\n",
    "    \"possibly_sensitive\",\n",
    "    \"referenced_tweets\",\n",
    "    \"reply_settings\",\n",
    "    \"source\",\n",
    "    \"text\",\n",
    "]\n",
    "\n",
    "paginator = tweepy.Paginator(\n",
    "    client.search_all_tweets,\n",
    "    query=query,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    expansions=expansions,\n",
    "    place_fields=place_fields,\n",
    "    user_fields=user_fields,\n",
    "    media_fields=media_fields,\n",
    "    tweet_fields=tweet_fields,\n",
    "    max_results=500,\n",
    ")\n",
    "\n",
    "frames = {\n",
    "    \"tweet\": [],\n",
    "    \"user\": [],\n",
    "    \"media\": [],\n",
    "    \"place\": [],\n",
    "    \"referenced\": [],\n",
    "    \"hashtag\": [],\n",
    "    \"url\": []\n",
    "}\n",
    "\n",
    "for response in paginator:\n",
    "    # Extract entities (hashtags, urls), and referenced_tweets as their own dataframes\n",
    "    hashtags_df, urls_df = extract_entities(response.data)\n",
    "    referenced_df = extract_referenced(response.data)\n",
    "\n",
    "    # Remove entities and referenced tweets from the response data\n",
    "    #  we have already \"saved\" it in the above 2 lines\n",
    "    tweet_records = [tweet.data for tweet in response.data]\n",
    "    for tweet_data in tweet_records:\n",
    "        if \"entities\" in tweet_data:\n",
    "            tweet_data.pop(\"entities\")\n",
    "        if \"referenced_tweets\" in tweet_data:\n",
    "            tweet_data.pop(\"referenced_tweets\")\n",
    "\n",
    "    # Convert the json data for dataframe\n",
    "    tweet_df = pd.json_normalize(tweet_records)\n",
    "\n",
    "    # Get the many to many relationship between id and media keys\n",
    "    tweet_media_links = (\n",
    "        tweet_df[[\"id\", \"attachments.media_keys\"]].explode(\"attachments.media_keys\")\n",
    "        .rename(columns={\"id\": \"tweet_id\", \"attachments.media_keys\": \"media_key\"})\n",
    "    )\n",
    "\n",
    "    # Drop media_keys from tweet_df\n",
    "    tweet_df = tweet_df.drop(\"attachments.media_keys\", axis=1)\n",
    "    users_df = pd.json_normalize([user.data for user in response.includes[\"users\"]])\n",
    "\n",
    "    # Sometimes the response doesn't contain any media objects:\n",
    "    media_df = None\n",
    "    if \"media\" in response.includes:\n",
    "        # Store the media_keys along with their corresponding tweet_ids into\n",
    "        #  the media_df.\n",
    "        # This means tweet_df will NOT have a media_key column, and instead\n",
    "        #  media_df will contain the mappings to tweet_df ids.\n",
    "        media_df = (\n",
    "            pd.json_normalize([media.data for media in response.includes[\"media\"]])\n",
    "            .merge(tweet_media_links, on=\"media_key\")\n",
    "        )\n",
    "\n",
    "    # Sometimes the response doesn't contain any place objects\n",
    "    place_df = None\n",
    "    if \"places\" in response.includes:\n",
    "        place_df = pd.json_normalize([place.data for place in response.includes[\"places\"]])\n",
    "\n",
    "    # Append the frames from the current response to our dictionary,\n",
    "    #  so we can combine everything later\n",
    "    frames[\"tweet\"].append(tweet_df)\n",
    "    frames[\"user\"].append(users_df)\n",
    "    frames[\"hashtag\"].append(hashtags_df)\n",
    "    frames[\"url\"].append(urls_df)\n",
    "    frames[\"referenced\"].append(referenced_df)\n",
    "    if media_df is not None:\n",
    "        frames[\"media\"].append(media_df)\n",
    "    if place_df is not None:\n",
    "        frames[\"place\"].append(place_df)\n",
    "\n",
    "    # sleep for 1 second to avoid TooManyRequests error\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create an output data folder\n",
    "data_path = pathlib.Path(__file__).parent / \"data\"\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Empty out old results from data folder\n",
    "for out_file in data_path.glob(\"*.csv\"):\n",
    "    out_file.unlink()\n",
    "\n",
    "# Combine the dataframe lists from our frames dictionary, \n",
    "#  then save those dataframes inside of the data folder\n",
    "for key, frame_list in frames.items():\n",
    "    if frame_list:\n",
    "        df = pd.concat(frame_list)\n",
    "        df.to_csv(data_path / f\"{key}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
