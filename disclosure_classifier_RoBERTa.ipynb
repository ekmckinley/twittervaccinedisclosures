{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtAqCjPx8-RM"
      },
      "outputs": [],
      "source": [
        "# Install required packages for the project\n",
        "!pip install transformers demoji\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import re\n",
        "import demoji\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import files\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Download emoji codes for preprocessing\n",
        "demoji.download_codes()\n",
        "\n",
        "# Load dataset uploaded by the user\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"All Human Coded Disclosure Tweets.csv\")\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses input text by lowercasing, removing punctuation, URLs, emojis,\n",
        "    smileys, numbers, email addresses, and replacing certain patterns.\"\"\"\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[#.,!?]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = demoji.replace(text, '')  # Remove emojis\n",
        "    text = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)  # Remove smileys\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove email addresses\n",
        "    text = text.replace('&amp', 'and')  # Replace &amp with 'and'\n",
        "    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', text)  # Remove dates in YYYY-MM-DD format\n",
        "    return text.strip()\n",
        "\n",
        "# Preprocess texts and retain relevant columns\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "columns_to_keep = [\"processed_text\", \"coding (0 or 1)\"]\n",
        "df = df[[col for col in df.columns if col in columns_to_keep]]\n",
        "df.rename(columns={'processed_text': 'text', 'coding (0 or 1)': 'label'}, inplace=True)\n",
        "df.dropna(how=\"any\", inplace=True)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Define a PyTorch Dataset class\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom Dataset class for loading data into the model.\"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.labels = df['label'].values\n",
        "        self.texts = [tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "                      for text in df['text']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val.squeeze(0) for key, val in self.texts[idx].items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Split the dataframe into training, validation, and test sets\n",
        "np.random.seed(112)\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "# Define the RoBERTa classifier model\n",
        "class RoBERTaClassifier(nn.Module):\n",
        "    \"\"\"Classifier model based on RoBERTa.\"\"\"\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super(RoBERTaClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 2)  # RoBERTa base produces 768-dimensional vectors\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "        return final_layer\n",
        "\n",
        "# Training function\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "    \"\"\"Trains the model on the training dataset and evaluates on the validation dataset.\"\"\"\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(Dataset(train_data), batch_size=16, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(Dataset(val_data), batch_size=16)\n",
        "\n",
        "    # Setup\n",
        "    device = torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Create test loader\n",
        "test_loader = torch.utils.data.DataLoader(Dataset(df_test), batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluate(model, test_loader, device)\n"
      ],
      "metadata": {
        "id": "qDPiMbPA9vAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'roberta_classifier_model.pth')\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "kAbQQncU94V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the full dataset\n",
        "uploaded_full_dataset = files.upload()  # This will prompt you to upload a file\n",
        "full_df = pd.read_csv(next(iter(uploaded_full_dataset)))  # Replace with the actual file name if known\n",
        "\n",
        "# Preprocess the full dataset\n",
        "full_df['processed_text'] = full_df['text'].apply(preprocess_text)\n",
        "print(\"Full dataset preprocessed.\")\n"
      ],
      "metadata": {
        "id": "559SsV6999Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the full dataset for model input\n",
        "full_dataset = Dataset(full_df)\n",
        "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32)\n",
        "\n",
        "# Classify the full dataset\n",
        "model.eval()  # Make sure model is in evaluation mode\n",
        "full_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in full_loader:\n",
        "        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        full_predictions.extend(preds.tolist())\n",
        "\n",
        "full_df['predicted_label'] = full_predictions\n",
        "print(\"Classification complete.\")\n"
      ],
      "metadata": {
        "id": "o9IVChUC9_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"classified_dataset.csv\"\n",
        "full_df.to_csv(output_filename, index=False)\n",
        "files.download(output_filename)  # This will download the file to your local machine\n",
        "print(f\"Output saved to {output_filename} and download initiated.\")\n"
      ],
      "metadata": {
        "id": "UqH581SB-BKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}